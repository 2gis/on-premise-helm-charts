apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ include "bss-receiver-streams.name" . }}
  labels:
    {{- include "bss-receiver-streams.labels" . | nindent 4 }}
data:
  application.conf: |
    include classpath("application")

    streams = {
      topology = {
        blacklist.enabled = false
        
        deduplication = {
          in-memory = true
    
          retention = {
            period = 1 hour
            window = 10 minutes
          }
        }
    
        schema-loader = {
          v-1-schemas-path = "file:///app/schema/v1/"
          v-2-schemas-path = "file:///app/schema/v2/"
          v-3-schemas-path = "file:///app/schema/v3/0/"
        }
    
        initialize-topics = {{ required "A valid .Values.initializeTopics entry required" .Values.initializeTopics }}
        topics-manager.prefix = {{ required "A valid .Values.topicsPrefix entry required" .Values.topicsPrefix }}
    
        input-topics = [
          "{{ required "A valid .Values.topicsPrefix entry required" .Values.topicsPrefix }}input"
        ]
        invalid-message-topic = "{{ required "A valid .Values.topicsPrefix entry required" .Values.topicsPrefix }}invalid-messages"
        unknown-type-topic = "{{ required "A valid .Values.topicsPrefix entry required" .Values.topicsPrefix }}unknown-type"

        types-topics = [
          { topic = "{{ required "A valid .Values.topicsPrefix entry required" .Values.topicsPrefix }}type.401", type = 401 }
        ]

        segment-info-index.data-path = "file:///app/segment_info/"
      }

      kafka-streams = {
        "application.id" = "bss.receiver.streams"
        "group.instance.id" = "bss.receiver.streams"
        "session.timeout.ms" = 300000
        "transaction.timeout.ms" = 180000
        "processing.guarantee" = "exactly_once"
        "commit.interval.ms" = 5000
        "replication.factor" = 3
        "num.stream.threads" = 1
        "state.dir" = "/app/rocksdb"
        "cache.max.bytes.buffering" = 268435456 # 256Mb
        "rocksdb.info.log.level" = ERROR_LEVEL
        "rocksdb.shared.block.cache.size" = 536870912 # 512Mb
        "bootstrap.servers" = {{ required "A valid .Values.kafka.servers entry required" .Values.kafka.servers | quote }}
        {{- if .Values.kafka.sasl.mechanism }}
        "sasl.mechanism" = {{ .Values.kafka.sasl.mechanism }}
        {{- end }}
        {{- if .Values.kafka.security.protocol }}
        "security.protocol" = {{ .Values.kafka.security.protocol }}
        {{- end }}
        {{- if .Values.kafka.sasl.jaas.config }}
        "sasl.jaas.config" = {{ .Values.kafka.sasl.jaas.config }}
        {{- end }}
        "client.id" = "bss.receiver.streams"
        "metrics.recording.level" = DEBUG
        "max.request.size" = 10485760
        "linger.ms" = 100
        "acks" = "all"
        "compression.type" = "gzip"
        "batch.size" = 65536
        "rocksdb.config.setter" = "casino.bss.receiver.streams.StreamsRocksDBConfigSetter"
        "windowstore.changelog.additional.retention.ms" = 0
      }
    }

  logback.xml: |
    <configuration>
      <appender name="STDOUT" class="ch.qos.logback.core.ConsoleAppender">
        <encoder class="net.logstash.logback.encoder.LogstashEncoder">
          <throwableConverter class="net.logstash.logback.stacktrace.ShortenedThrowableConverter">
            <maxDepthPerThrowable>30</maxDepthPerThrowable>
            <maxLength>28672</maxLength> <!-- 28Kb for stacktrace, because full json ELK limit is 32Kb-->
            <shortenedClassNameLength>20</shortenedClassNameLength>
            <exclude>scala\.concurrent\.Future.*</exclude>
            <exclude>scala\.concurrent\.forkjoin..*</exclude>
            <rootCauseFirst>true</rootCauseFirst>
          </throwableConverter>
        </encoder>
      </appender>

      <root level="INFO">
        <appender-ref ref="STDOUT" />
      </root>
    </configuration>
